# -*- coding: utf-8 -*-
"""CSE6242 Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iQZqM1PkjuRWvxKm5SOtlck2fJmkD9jg

#Spotify Hit Predictor

## Preliminaries

Set the environment & manage imports
"""

!pip install -q umap-learn
!gdown 1YDEl9mA3Vr7zGZtkMeouexCNHhFaa_kn
!pip install scikit-learn==1.4.2

# imports
# from google.colab import drive
# drive.mount('/content/drive')
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import decomposition, cluster
from sklearn.preprocessing import MinMaxScaler
#import umap
import os

# Create pandas dataframe
df = pd.read_csv("genre_music.csv")

# Summary statistics for numerical columns
df.describe()

"""# Data Exploration"""

# Check for missing values
# print(df.isnull().sum())

# Check the distribution of genres
print(df['genre'].value_counts())

# Visualize the distribution of numerical features
df.hist(bins=30, figsize=(20,15))
plt.show()

"""## Data Preprocessing

From the sample, genre, decade, artist, and track are **categorical** variables. We typically do not encode the artist and track fields since they are unique identifiers, but genre and decade could be useful for our analysis.
"""

# Count of duplicates
print("Dropped", len(df[df.duplicated(keep='first')]), "duplicate rows!")

# Drop duplicates:
df.drop_duplicates(inplace=True)

# Reset the index if needed
df = df.reset_index(drop=True)

# Finding unique genres
unique_genres = set(df['genre'])
print("Unique Genres: ", unique_genres)
print("Number of Unique Genres: ", len(unique_genres))

import seaborn as sns

# Choose a numerical feature for the boxplot
# temp: energy
numerical_feature = 'energy'

# Boxplot for the distribution of a numerical feature across genres
plt.figure(figsize=(12, 6))
sns.boxplot(x='genre', y=numerical_feature, data=df)
plt.title(f'Distribution of {numerical_feature.capitalize()} Across Genres')
plt.xticks(rotation=45, ha='right')
plt.show()

# Boxplot for the distribution of a numerical feature across decades
plt.figure(figsize=(12, 6))
sns.boxplot(x='decade', y=numerical_feature, data=df)
plt.title(f'Distribution of {numerical_feature.capitalize()} Across Decades')
plt.xticks(rotation=45, ha='right')
plt.show()

# Genre Distribution

# Check the distribution of genres after preprocessing
print(df['genre'].value_counts())

from collections import Counter

# Count occurrences of each genre
genre_counts = dict(Counter(df['genre']))
print("Genre Counts: ", genre_counts)

# Create histogram
plt.bar(genre_counts.keys(), genre_counts.values())

# Label axes and add a title
plt.xlabel('Genres')
plt.ylabel('Song Count')
plt.title('Histogram of the number of songs of each genre:')

# Adjust appearance of x-axis labels
plt.xticks(rotation=45, ha='right')

plt.show()

# Decade Distribution

# Check distribution of decades after preprocessing
print(df['decade'].value_counts())

# Count occurrences of each decade
decade_counts = dict(Counter(df['decade']))
print("Decade Counts: ", decade_counts)

# Create histogram
plt.bar(decade_counts.keys(), decade_counts.values())

# Label axes and add a title
plt.xlabel('Decades')
plt.ylabel('Song Count')
plt.title('Histogram of the Number of Songs in Each Decade')

# Adjust appearance of x-axis labels
plt.xticks(rotation=45, ha='right')

plt.show()

df
df.to_csv("genre_updated_names.csv")

try:
  df = df.drop(['track','artist','duration_s'], axis=1)
except:
  pass
df.head()

df
df.to_csv("genre_updated.csv")

from sklearn.preprocessing import StandardScaler

# Initializing the StandardScaler
#scaler = StandardScaler()

# Assuming you want to scale all numerical columns except the last three, which are ('genre', 'decade', and 'popularity')
df_to_scale = df.iloc[:,:-3]

# Selecting only numerical columns for scaling
# df_numeric = df_to_scale.select_dtypes(include=[np.number])

# Scale the numerical data
#scaled_array = scaler.fit_transform(df_numeric.values)

# Convert the scaled data back to a DataFrame
#df_processed = pd.DataFrame(scaled_array, columns=df_numeric.columns)
df_processed = df_to_scale

# Adding the 'genre' column back to the processed DataFrame
df_processed['genre'] = df['genre']
df_processed['popularity'] = df['popularity']

# Display the first few rows of the processed DataFrame
df_processed.head()

# import joblib
# joblib.dump(scaler, "standard_scaler.joblib")

expanded_df = df_processed.explode('genre')

# Calculate the mean values for each genre
# Assuming all columns except 'genre' are numerical and we want to include them in the pivot table
numerical_columns = [col for col in expanded_df.columns if col != 'genre']
df_genre_means = pd.pivot_table(expanded_df, index='genre', values=numerical_columns, aggfunc='mean').reset_index()

# Printing the means
print("Mean values of each feature for every genre in the dataframe.\n".center(150, " "))
df_genre_means

import seaborn as sns
import matplotlib.pyplot as plt

corr_mat = df_processed[numerical_columns].corr()

# Generate a heatmap in seaborn
plt.subplots(figsize=(12,9))
sns.heatmap(corr_mat, cmap='coolwarm', annot=True)

# Title for the heatmap
plt.title('\nCorrelation between Features:\n', fontsize=20)

# Display the heatmap
plt.show()

# Prints out highly correlated features
thresholds = [0.8, 0.6, 0.4]  # Adjust the threshold as needed

# Took upper triangular part of the correlation matrix
upper_triangular = np.triu(corr_mat, k=1)

# Highly correlated feature pairs
for threshold in thresholds:
  high_corr_pairs = [(corr_mat.columns[i], corr_mat.columns[j]) for i in range(upper_triangular.shape[0]) \
                     for j in range(i+1, upper_triangular.shape[1]) if abs(upper_triangular[i, j]) > threshold]

  print(threshold, high_corr_pairs)

"""### Reflections

Moderate correlation at 0.6: **energy** correlates w/ **loudness** and acousticness, which also makes sense intuitively.


The correlation doesn't seem to be high between features and so we will move forward with all the features.

### Splitting Data

We will now split the data into the training (80%) and the test set (20%). We will use the **genre** column class labels to classify the songs.
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
import pandas as pd

# The target variable ('y') is the last column, and the rest are feature columns ('X')

X = df_processed.iloc[:,:-2]  # Features: All columns except the last two
y = df_processed.iloc[:,-1]   # Target: The last column

# Splitting the dataset into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)

# Initialize the RobustScaler
scaler = RobustScaler()

# This avoids data leakage from the test set into the model training process
scaler.fit(x_train)
x_train_scaled = scaler.transform(x_train)
x_test_scaled = scaler.transform(x_test)

cols = X.columns
x_train_scaled = pd.DataFrame(x_train_scaled, columns=cols)
x_test_scaled = pd.DataFrame(x_test_scaled, columns=cols)

# Display the summary statistics of the scaled training data to check the scaling
x_train_scaled.describe()

joblib.dump(scaler, "genre_scaler.joblib")

x_test_scaled.describe()

from sklearn.preprocessing import LabelEncoder
import pandas as pd

# Initialize LabelEncoder
LE = LabelEncoder()

# Encode the target variables
y_train_encoded = LE.fit_transform(y_train)
y_test_encoded = LE.transform(y_test)

y_train_original = LE.inverse_transform(y_train_encoded)
y_test_original = LE.inverse_transform(y_test_encoded)

y_original = pd.concat([pd.DataFrame(y_train_original), pd.DataFrame(y_test_original)], axis=0)

label_mapping = dict(zip(LE.transform(LE.classes_), LE.classes_))

print(label_mapping)

"""## Dimensionality Reduction

## PCA

Now we will be running PCA on the dataframe in order to reduce the number of dimensions and remove highly correlated data while retaining most information.

"""

# # calculate PCA
# df_processed_PCA = df_processed.copy()
# df_PCA_analysis = df_processed_PCA[numerical_columns]
# print("this is inital shape", df_PCA_analysis.shape[0])

# df_PCA_analysis = df_PCA_analysis.drop(['popularity'], axis=1)


# #print(df_PCA_analysis)
# scaler = MinMaxScaler()
# x_normalized = scaler.fit_transform(df_PCA_analysis)

# pca = decomposition.PCA(n_components=3)
# results = pca.fit_transform(x_normalized)
# explained_variance = pca.explained_variance_ratio_
# #print(results)
# #print(type(results))
# #print("this is final shape", results.shape[0])
# print("This is the variance", explained_variance)

# ## plots a sample PCA

# PCA_1 = results[:, 0].reshape(-1, 1)
# PCA_2 = results[:, 1].reshape(-1, 1)
# PCA_3 = results[:, 2].reshape(-1, 1)
# scaler = MinMaxScaler()
# PCA_1_normalized = scaler.fit_transform(PCA_1)
# PCA_2_normalized = scaler.fit_transform(PCA_2)

# plt.scatter(PCA_2,PCA_1)
# plt.show()
# plt.close()

# from mpl_toolkits.mplot3d import Axes3D
# from sklearn.preprocessing import LabelEncoder


# LE = LabelEncoder()
# y_encoded = LE.fit_transform(df['genre'])
# y_original = df['genre']
# # Create3D scatter plot
# fig = plt.figure(figsize=(10, 8))
# ax = fig.add_subplot(111, projection='3d')

# scatter = ax.scatter(PCA_3, PCA_1, PCA_2, c=y_encoded, cmap='viridis')

# # Use the label encoder to find the unique original labels
# unique_y_encoded = np.unique(y_encoded)
# unique_y_original = LE.inverse_transform(unique_y_encoded)

# # Create a colorbar with original labels
# cbar = plt.colorbar(scatter, ax=ax, ticks=unique_y_encoded)
# cbar.set_ticklabels(unique_y_original)
# cbar.set_label('Genre')

# # Add labels and title
# ax.set_xlabel('PCA Component 1')
# ax.set_ylabel('PCA Component 2')
# ax.set_zlabel('PCA Component 3')
# plt.title('PCA 3D Scatter Plot')
# ax.view_init(elev=20, azim=60)

# plt.show()

from sklearn.decomposition import PCA

cov_matrix_PCA = PCA(n_components=len(X.columns))
cov_matrix_PCA.fit(X)

# from sklearn.preprocessing import LabelEncoder
# from sklearn.decomposition import PCA
# import matplotlib.pyplot as plt
# import numpy as np
# from mpl_toolkits.mplot3d import Axes3D
# import seaborn as sns
# from pandas.plotting import parallel_coordinates

# # Assuming 'y_original' contains the original categorical labels
# LE = LabelEncoder()
# y_encoded = LE.fit_transform(y_original)

# # Now running PCA with 8 principal components
# pca = PCA(n_components=8)
# X_PCA = pca.fit_transform(X)

# # Creating a DataFrame with the PCA results and the associated labels
# df_pca = pd.DataFrame(X_PCA, columns=[f'PC{i+1}' for i in range(8)])
# df_pca['Label'] = LE.inverse_transform(y_encoded)  # Adding the original labels back for coloring

# # Creating the parallel coordinates plot
# plt.figure(figsize=(12, 6))
# parallel_coordinates(df_pca, 'Label', colormap='rainbow', alpha=0.5)
# plt.title('Parallel Coordinates Plot for PCA Components')
# plt.xlabel('PCA Components')
# plt.ylabel('Component Values')
# plt.legend(loc='upper right', bbox_to_anchor=(1.15, 1))
# plt.grid(True)
# plt.show()

# # Create a 3D scatter plot
# fig = plt.figure(figsize=(10, 8))
# ax = fig.add_subplot(111, projection='3d')

# # Defined a colormap for assigning colors based on the encoded labels
# cmap = plt.get_cmap('tab10')

# # Scatter plot with the first three PCA components, using the encoded 'y'
# scatter = ax.scatter(X_PCA[:, 0], X_PCA[:, 1], X_PCA[:, 2], c=y_encoded, cmap=cmap)

# # Creating a colorbar with labels based on unique classes
# # Mapping the encoded labels back to the original categorical labels for the colorbar
# unique_y_encoded = np.unique(y_encoded)
# cbar = plt.colorbar(scatter, ax=ax, ticks=unique_y_encoded)
# cbar.set_ticklabels(LE.inverse_transform(unique_y_encoded))
# cbar.set_label('Predicted Labels')

# # Add labels and title to the plot
# ax.set_xlabel('PCA Component 1')
# ax.set_ylabel('PCA Component 2')
# ax.set_zlabel('PCA Component 3')
# plt.title('PCA 3D Scatter Plot:')
# ax.view_init(elev=20, azim=60)

# plt.show()

"""## UMAP

"""

### DO NOT RUN CODE #####################
### Code used to generate the UMAPS dimension reduction: takes a while to run!!########
"""
df_processed_UMAPS = df_processed.copy()
df_processed_UMAPS_numerical = df_processed_UMAPS[numerical_columns].to_numpy()


X = df_processed_UMAPS_numerical
scaler = MinMaxScaler()
x_normalized = scaler.fit_transform(X)
#min_dists = [.1, .25, .5, .8, .99]
#n_neighbors = [20, 50, 100, 200] # 500 is too much
min_dists = [.1,0.5,.99]
n_neighbors = [20,100,200]
n_components = 2
metric='cosine'

def make_umap(n_neighbor, min_dist):
    ids = []
    xs = []
    ys = []
    nneighbors = []
    mdist = []
    ncomponents = []
    metrics = []
    category = []
    dlength = len(x_normalized)

    print(f"neighbors = {n_neighbor}, dist = {min_dist}")
    mapper = umap.UMAP(n_neighbors=n_neighbor,
            min_dist=min_dist,
            n_components=n_components,
            metric=metric).fit_transform(x_normalized)
    x = list(mapper[:,0])
    y = list(mapper[:,1])
    xs += x
    ys += y
    #ids += df.pol_id.to_list()
    nneighbors += [n_neighbor]*dlength
    mdist += [min_dist]*dlength
    ncomponents += [n_components]*dlength
    metrics += [metric]*dlength
    #category += df.category.to_list()

    results = pd.DataFrame({
        'x': xs,
        'y': ys,
        'n_neighbors': nneighbors,
        'min_dist': mdist,
        'n_components': ncomponents,
        'metric': metrics,
    })
    print(f"done: neighbors = {n_neighbor}, dist = {min_dist}")
    results.to_csv(f'/content/drive/MyDrive/umaps/data/umaps_nn_{n_neighbor}_md_{min_dist}.csv', index=False)
    return results

args = []
for n_neighbor in n_neighbors:
    for min_dist in min_dists:
        args.append((n_neighbor, min_dist))

print(args[0])

for arg in args:
  print(arg[0], arg[1])
  make_umap(arg[0], arg[1])
"""

# #PLOT UMAPS
# files_names = os.listdir('/content/drive/MyDrive/umaps/data/')
# print(files_names)
# for iter in files_names:
#   df_temp = pd.read_csv(f"/content/drive/MyDrive/umaps/data/{iter}")
#   plt.scatter(df_temp['x'].to_numpy(),df_temp['y'].to_numpy() )
#   plt.tick_params(left = False, right = False , labelleft = False ,
#                 labelbottom = False, bottom = False)
#   plt.savefig(f"/content/drive/MyDrive/umaps/umaps_fig/{iter}.png")
#   plt.close()

"""## LDA"""

# LDA

# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
# cov_matrix_LDA = LinearDiscriminantAnalysis(n_components=len(np.unique(y_original)) - 1)
# print(len(np.unique(y_original)) - 1)
# X_LDA = cov_matrix_LDA.fit_transform(X, y.values.ravel())

# # Create a 3D scatter plot using y_encoded for coloring
# fig = plt.figure(figsize=(10, 8))
# ax = fig.add_subplot(111, projection='3d')

# # Defined a colormap for assigning colors based on labels
# cmap = plt.get_cmap('tab10')

# # Use y_encoded for the c argument
# scatter = ax.scatter(X_LDA[:, 0], X_LDA[:, 1], X_LDA[:, 2], c=y_encoded, cmap=cmap)

# # Since y_encoded is numerical, we use it directly for ticks in the colorbar
# # And use LE.inverse_transform to get the original string labels for the colorbar
# cbar = plt.colorbar(scatter, ax=ax, ticks=np.unique(y_encoded))
# cbar.set_ticklabels(LE.inverse_transform(np.unique(y_encoded)))
# cbar.set_label('Predicted Labels')

# # Add labels and title
# ax.set_xlabel('LDA Component 1')
# ax.set_ylabel('LDA Component 2')
# ax.set_zlabel('LDA Component 3')
# plt.title('LDA 3D Scatter Plot:')
# ax.view_init(elev=20, azim=60)

# plt.show()

"""## Clustering

K-means
"""

# kmeans_data = X_LDA[:, 0:3]

# n_clusters = 6
# kmeans = cluster.KMeans(n_clusters=n_clusters).fit(kmeans_data)

# fig = plt.figure(figsize=(10, 8))
# ax = fig.add_subplot(111, projection='3d')

# ax.scatter(X_LDA[:, 0], X_LDA[:, 1], X_LDA[:, 2], c=y_encoded, cmap='viridis')
# cbar = plt.colorbar(scatter, ax=ax, ticks=unique_y_encoded)
# cbar.set_ticklabels(unique_y_original)
# cbar.set_label('Genre')

# centroids = kmeans.cluster_centers_
# ax.scatter(
#     centroids[:, 0],
#     centroids[:, 1],
#     centroids[:, 2],
#     marker="x",
#     # s=169,
#     linewidths=20,
#     color="b",
#     # zorder=10,
# )

# # Add labels and title
# ax.set_xlabel('LDA Component 1')
# ax.set_ylabel('LDA Component 2')
# ax.set_zlabel('LDA Component 3')
# plt.title('KMeans Clusters Visualization')
# ax.view_init(elev=20, azim=60)

# plt.show()

# # Elbow method
# from sklearn.metrics import silhouette_samples, silhouette_score

# cluster_sizes = list(range(2, 9, 1))
# silhouette_averages = []
# for n_clusters in cluster_sizes:
#   kmeans = cluster.KMeans(n_clusters=n_clusters)
#   labels = kmeans.fit_predict(kmeans_data)
#   silhouette_avg = silhouette_score(kmeans_data, labels)
#   silhouette_averages.append(silhouette_avg)

# plt.plot(cluster_sizes, silhouette_averages)
# plt.show()

# inertia = []
# distortions = []
# for n_clusters in cluster_sizes:
#   kmeans = cluster.KMeans(n_clusters=n_clusters)
#   labels = kmeans.fit_predict(kmeans_data)
#   inertia.append(kmeans.inertia_)
#   distortions.append(kmeans.inertia_ / kmeans_data.shape[0])

# plt.plot(cluster_sizes, distortions, 'x-')
# plt.xlabel("Number of Clusters")
# plt.ylabel("Distortions")
# plt.title("Identifying Optimal Number of KMeans Clusters")

# for n_clusters in range(2, 9, 1):
#   kmeans = cluster.KMeans(n_clusters=n_clusters)
#   kmeans.fit(df_processed[numerical_columns])

"""# Model Training and Evaluation

Random Forest, Naive Bayes, SGD

### Training Data

We will run our 3 ML models (Random Forest, Naive Bayes, SGD) on our training data
"""

from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.multiclass import OneVsOneClassifier
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.preprocessing import LabelEncoder
import numpy as np
import warnings

# Preparing models for comparison
sgd_clf = OneVsOneClassifier(SGDClassifier(loss='log_loss', max_iter=5000, shuffle=True))
models = [
    ['Naive Bayes', GaussianNB()],
    ['SGD', sgd_clf]
]

# Random Forest model setup
RF_clf = RandomForestClassifier(random_state=42, min_samples_split=5)

# Initialize StratifiedKFold for cross-validation
kfold = StratifiedKFold(n_splits=10, random_state=13, shuffle=True)

# Evaluate Random Forest and other models
results = []
print("Evaluating Random Forest:")
RF_cross_val_score = cross_val_score(RF_clf, x_train_scaled, y_train, cv=kfold, scoring='accuracy')
results.append(('Random Forest', RF_cross_val_score))
print("CrossVal Score Mean: ", RF_cross_val_score.mean())
print("CrossVal Score Std: ", RF_cross_val_score.std())

# Evaluate other models
for name, model in models:
    cv_score = cross_val_score(model, x_train_scaled, y_train, cv=kfold, scoring='accuracy')
    results.append((name, cv_score))
    print(f"\n{name} ->")
    print("CrossVal Score Mean: ", cv_score.mean())
    print("CrossVal Score Std: ", cv_score.std())

"""### Training Data Evaluation

It seems likeall of our models have good accuracy. We can see that Random Forest Classifier has the highest accuracy (87.8%).
"""

from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.model_selection import cross_val_predict

res_precision_recall = []

# Random Forest Evaluation
y_pred_RF = cross_val_predict(RF_clf, x_train_scaled, y_train, cv=10)
RF_precision = precision_score(y_train, y_pred_RF, average="weighted")
RF_recall = recall_score(y_train, y_pred_RF, average="weighted")
RF_F1 = f1_score(y_train, y_pred_RF, average="weighted")
res_precision_recall += [['Random Forest', RF_precision, RF_recall, RF_F1]]
print('%s -> %s: %f     %s: %f    %s: %f' % ('Random Forest', 'Precision Score', RF_precision, 'Recall Score', RF_recall, 'F1 Score', RF_F1))

# Evaluating other models
for name, model in models:
    y_pred = cross_val_predict(model, x_train_scaled, y_train, cv=kfold)
    precision = precision_score(y_train, y_pred, average="weighted")
    recall = recall_score(y_train, y_pred, average="weighted")
    model_f1_score = f1_score(y_train, y_pred, average="weighted")
    res_precision_recall += [[name, precision, recall, model_f1_score]]
    print('%s -> %s: %f     %s: %f    %s: %f' % (name, 'Precision Score', precision, 'Recall Score', recall, 'F1 Score', model_f1_score))

"""Random Forest Classifier has the best F1 score followed by SGD Classifier, and Naive Bayes.

# Model Comparison

We can use ths section to compare performance of different models
"""

import numpy as np
import matplotlib.pyplot as plt

# Extracting scores
models_names = ['Random Forest', 'Naive Bayes', 'SGD']
precision_scores = [score[1] for score in res_precision_recall if score[0] in models_names]
recall_scores = [score[2] for score in res_precision_recall if score[0] in models_names]
F1_scores = [score[3] for score in res_precision_recall if score[0] in models_names]

# Plotting
width = 0.3
x = np.arange(len(models_names))

plt.bar(x - width, F1_scores, width, label='F1 Score')
plt.bar(x, precision_scores, width, label='Precision')
plt.bar(x + width, recall_scores, width, label='Recall')

plt.xlabel("Models")
plt.ylabel("Scores")
plt.title("Comparison of Model Performance (Training)")
plt.xticks(x, models_names)
plt.legend()
plt.show()

"""Now we will run it on testing data"""

# Evaluation on testing data
precision_scores = []
recall_scores = []
F1_scores = []

# Random Forest Evaluation
RF_clf.fit(x_train_scaled, y_train)
RF_y_pred_test = RF_clf.predict(x_test_scaled)

# Calculate precision, recall, and F1 scores
RF_precision_test = precision_score(y_test, RF_y_pred_test, average='weighted', zero_division=1)
RF_recall_test = recall_score(y_test, RF_y_pred_test, average='weighted')
RF_F1_test = f1_score(y_test, RF_y_pred_test, average='weighted')

precision_scores.append(RF_precision_test)
recall_scores.append(RF_recall_test)
F1_scores.append(RF_F1_test)
print("Random Forest Precision Score (Testing): ", RF_precision_test)
print("Random Forest Recall Score (Testing): ", RF_recall_test)
print('Random Forest F1 Score (Testing): ', RF_F1_test)

# Evaluating other models on testing data
for name, model in models:
    model.fit(x_train_scaled, y_train)  # Fit the model with training data
    model_pred_test = model.predict(x_test_scaled)  # Predict on testing data

    # Calculate precision, recall, and F1 scores
    precision_test = precision_score(y_test, model_pred_test, average='weighted', zero_division=1)
    recall_test = recall_score(y_test, model_pred_test, average='weighted')
    F1_test = f1_score(y_test, model_pred_test, average='weighted')

    # Adding scores to the lists
    precision_scores.append(precision_test)
    recall_scores.append(recall_test)
    F1_scores.append(F1_test)

    # Printing scores
    print("")
    print(name, "Precision Score (Testing): ", precision_test)
    print(name, "Recall Score (Testing): ", recall_test)
    print(name, "F1 Score (Testing): ", F1_test)

x_train_scaled.describe()

import numpy as np
import matplotlib.pyplot as plt

# Adjusted width and x values for 3 models
width = 0.3
x = np.arange(3)

# Plotting
plt.bar(x - width, F1_scores, width, label='F1 Score')
plt.bar(x, precision_scores, width, label='Precision')
plt.bar(x + width, recall_scores, width, label='Recall')

plt.xlabel("Models")
plt.ylabel("Scores")
plt.title("Comparison of Model Performance (Testing)")
plt.xticks(x, ['Random Forest', 'Naive Bayes', 'SGD'])
plt.legend()
plt.show()

import joblib
joblib.dump(RF_clf, 'random_forest.joblib')

"""# Training Model to Predict Popularity

## Split Data
"""

# df_processed_swapped = df_processed.copy()

# column1 = 'popularity'
# column2 = 'genre'
# colnames = df_processed_swapped.columns.tolist()
# index1, index2 = colnames.index(column1), colnames.index(column2)
# colnames[index2], colnames[index1] = colnames[index1], colnames[index2]
# df_processed_swapped = df_processed_swapped[colnames]
# df_processed_swapped

# Initializing the StandardScaler
df_processed_swapped = df_processed.copy()

#scaler = MinMaxScaler()

# Assuming you want to scale all numerical columns except the last one, which is ('genre' in this case)
df_to_scale = df_processed_swapped.iloc[:,:-1]

# Selecting only numerical columns for scaling
df_numeric = df_to_scale.select_dtypes(include=[np.number])

# Scale the numerical data
#scaled_array = scaler.fit_transform(df_numeric.values)

# Convert the scaled data back to a DataFrame
#df_processed_swapped = pd.DataFrame(scaled_array, columns=df_numeric.columns)

# Adding the 'genre' column back to the processed DataFrame
df_processed_swapped['genre'] = df['genre']

# Display the first few rows of the processed DataFrame
df_processed_swapped.head()

one_hot_encoded = pd.get_dummies(df_processed_swapped['genre'], prefix='one_hot', dtype = int)

# Concatenate the one-hot encoded columns with the original DataFrame
df_processed_swapped = pd.concat([df_processed_swapped, one_hot_encoded], axis=1)

# Drop the original column since it's now encoded
df_processed_swapped.drop('genre', axis=1, inplace=True)
dropped_column = df_processed_swapped.pop('popularity')
df_processed_swapped = pd.concat([df_processed_swapped, dropped_column], axis=1)

df_processed_swapped

df_processed_swapped.iloc[:, :-1]

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
import pandas as pd

# The target variable ('y') is the last column, and the rest are feature columns ('X')

X = df_processed_swapped.iloc[:,:-1]  # Features: All columns except the last one
y = df_processed_swapped.iloc[:,-1]   # Target: The last column

# Splitting the dataset into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)

# Initialize the RobustScaler
scaler = RobustScaler()

# This avoids data leakage from the test set into the model training process
scaler.fit(x_train)
x_train_scaled = scaler.transform(x_train)
x_test_scaled = scaler.transform(x_test)

cols = X.columns
x_train_scaled = pd.DataFrame(x_train_scaled, columns=cols)
x_test_scaled = pd.DataFrame(x_test_scaled, columns=cols)

# Display the summary statistics of the scaled training data to check the scaling
x_train_scaled.describe()

joblib.dump(scaler, "popularity_scaler.joblib")

x_test_scaled.describe()

from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, RandomForestRegressor
from sklearn.linear_model import SGDClassifier, SGDRegressor
from sklearn.naive_bayes import GaussianNB
from sklearn.multiclass import OneVsOneClassifier
from sklearn.model_selection import StratifiedKFold, cross_val_score, KFold
from sklearn.preprocessing import LabelEncoder
import numpy as np
import warnings

# Preparing models for comparison
sgd_rgr = SGDRegressor(loss='log_loss', max_iter=5000, shuffle=True)
models = [
    ['SGD', sgd_rgr]
]

# Random Forest model setup
RF_rgr = RandomForestRegressor(random_state=42, min_samples_split=5, min_samples_leaf = 2)

# Initialize KFold for cross-validation
kfold = KFold(n_splits=5, random_state=13, shuffle=True)

# Evaluate Random Forest and other models
results = []
print("Evaluating Random Forest:")
RF_cross_val_score = cross_val_score(RF_rgr, x_train_scaled, y_train, cv=kfold, scoring='neg_mean_squared_error')
results.append(('Random Forest', RF_cross_val_score))
print("CrossVal Score Mean: ", RF_cross_val_score.mean())
print("CrossVal Score Std: ", RF_cross_val_score.std())

from sklearn.linear_model import SGDClassifier, SGDRegressor
import sklearn

# Preparing models for comparison
sgd_rgr = SGDRegressor(loss='squared_error', max_iter=5000, shuffle=True)
models = [
    ['SGD', sgd_rgr]
]
print(sklearn.metrics.get_scorer_names())
# Evaluate other models
for name, model in models:
    cv_score = cross_val_score(model, x_train_scaled, y_train, cv=kfold, scoring='neg_root_mean_squared_error')
    results.append((name, cv_score))
    print(f"\n{name} ->")
    print("CrossVal Score Mean: ", cv_score.mean())
    print("CrossVal Score Std: ", cv_score.std())

from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.multiclass import OneVsOneClassifier
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.preprocessing import LabelEncoder
import numpy as np
import warnings

# Preparing models for comparison
sgd_clf = OneVsOneClassifier(SGDClassifier(loss='log_loss', max_iter=5000, shuffle=True))
models = [
    ['Naive Bayes', GaussianNB()],
    ['SGD', sgd_clf]
]

# Random Forest model setup
RF_clf = RandomForestClassifier(random_state=42, min_samples_split=5)

# Initialize StratifiedKFold for cross-validation
kfold = StratifiedKFold(n_splits=10, random_state=13, shuffle=True)

# Evaluate Random Forest and other models
results = []
print("Evaluating Random Forest:")
RF_cross_val_score = cross_val_score(RF_clf, x_train_scaled, y_train, cv=kfold, scoring='accuracy')
results.append(('Random Forest', RF_cross_val_score))
print("CrossVal Score Mean: ", RF_cross_val_score.mean())
print("CrossVal Score Std: ", RF_cross_val_score.std())

# Evaluate other models
for name, model in models:
    cv_score = cross_val_score(model, x_train_scaled, y_train, cv=kfold, scoring='accuracy')
    results.append((name, cv_score))
    print(f"\n{name} ->")
    print("CrossVal Score Mean: ", cv_score.mean())
    print("CrossVal Score Std: ", cv_score.std())

from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.model_selection import cross_val_predict

res_precision_recall = []

# Random Forest Evaluation
y_pred_RF = cross_val_predict(RF_clf, x_train_scaled, y_train, cv=10)
RF_precision = precision_score(y_train, y_pred_RF, average="weighted")
RF_recall = recall_score(y_train, y_pred_RF, average="weighted")
RF_F1 = f1_score(y_train, y_pred_RF, average="weighted")
res_precision_recall += [['Random Forest', RF_precision, RF_recall, RF_F1]]
print('%s -> %s: %f     %s: %f    %s: %f' % ('Random Forest', 'Precision Score', RF_precision, 'Recall Score', RF_recall, 'F1 Score', RF_F1))

# Evaluating other models
for name, model in models:
    y_pred = cross_val_predict(model, x_train_scaled, y_train, cv=kfold)
    precision = precision_score(y_train, y_pred, average="weighted")
    recall = recall_score(y_train, y_pred, average="weighted")
    model_f1_score = f1_score(y_train, y_pred, average="weighted")
    res_precision_recall += [[name, precision, recall, model_f1_score]]
    print('%s -> %s: %f     %s: %f    %s: %f' % (name, 'Precision Score', precision, 'Recall Score', recall, 'F1 Score', model_f1_score))

import numpy as np
import matplotlib.pyplot as plt

# Extracting scores
models_names = ['Random Forest', 'Naive Bayes', 'SGD']
precision_scores = [score[1] for score in res_precision_recall if score[0] in models_names]
recall_scores = [score[2] for score in res_precision_recall if score[0] in models_names]
F1_scores = [score[3] for score in res_precision_recall if score[0] in models_names]

# Plotting
width = 0.3
x = np.arange(len(models_names))

plt.bar(x - width, F1_scores, width, label='F1 Score')
plt.bar(x, precision_scores, width, label='Precision')
plt.bar(x + width, recall_scores, width, label='Recall')

plt.xlabel("Models")
plt.ylabel("Scores")
plt.title("Comparison of Model Performance (Training)")
plt.xticks(x, models_names)
plt.legend()
plt.show()

# Evaluation on testing data
precision_scores = []
recall_scores = []
F1_scores = []

# Random Forest Evaluation
RF_clf.fit(x_train_scaled, y_train)
RF_y_pred_test = RF_clf.predict(x_test_scaled)

# Calculate precision, recall, and F1 scores
RF_precision_test = precision_score(y_test, RF_y_pred_test, average='weighted', zero_division=1)
RF_recall_test = recall_score(y_test, RF_y_pred_test, average='weighted')
RF_F1_test = f1_score(y_test, RF_y_pred_test, average='weighted')

precision_scores.append(RF_precision_test)
recall_scores.append(RF_recall_test)
F1_scores.append(RF_F1_test)
print("Random Forest Precision Score (Testing): ", RF_precision_test)
print("Random Forest Recall Score (Testing): ", RF_recall_test)
print('Random Forest F1 Score (Testing): ', RF_F1_test)

# Evaluating other models on testing data
for name, model in models:
    model.fit(x_train_scaled, y_train)  # Fit the model with training data
    model_pred_test = model.predict(x_test_scaled)  # Predict on testing data

    # Calculate precision, recall, and F1 scores
    precision_test = precision_score(y_test, model_pred_test, average='weighted', zero_division=1)
    recall_test = recall_score(y_test, model_pred_test, average='weighted')
    F1_test = f1_score(y_test, model_pred_test, average='weighted')

    # Adding scores to the lists
    precision_scores.append(precision_test)
    recall_scores.append(recall_test)
    F1_scores.append(F1_test)

    # Printing scores
    print("")
    print(name, "Precision Score (Testing): ", precision_test)
    print(name, "Recall Score (Testing): ", recall_test)
    print(name, "F1 Score (Testing): ", F1_test)

joblib.dump(RF_clf, "popularity_random_forest.joblib")

import numpy as np
import matplotlib.pyplot as plt

# Adjusted width and x values for 3 models
width = 0.3
x = np.arange(3)

# Plotting
plt.bar(x - width, F1_scores, width, label='F1 Score')
plt.bar(x, precision_scores, width, label='Precision')
plt.bar(x + width, recall_scores, width, label='Recall')

plt.xlabel("Models")
plt.ylabel("Scores")
plt.title("Comparison of Model Performance (Testing)")
plt.xticks(x, ['Random Forest', 'Naive Bayes', 'SGD'])
plt.legend()
plt.show()

!pip install dash
!pip install dash-bootstrap-components
!pip install jupyter-dash
!pip install pyngrok

# Confusion Matrix

from sklearn.metrics import confusion_matrix
import seaborn as sns

# Random Forest on training data
y_pred_RF_train = cross_val_predict(RF_clf, x_train_scaled, y_train, cv=kfold)
cm_rf_train = confusion_matrix(y_train, y_pred_RF_train)

# Plotting confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(cm_rf_train, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix for Random Forest - Training')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

# Random Forest on testing data
RF_y_pred_test = RF_clf.predict(x_test_scaled)
cm_rf_test = confusion_matrix(y_test, RF_y_pred_test)

# Plotting confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(cm_rf_test, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix for Random Forest - Testing')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()